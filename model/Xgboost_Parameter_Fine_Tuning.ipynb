{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from itertools import product\n",
    "\n",
    "# my module\n",
    "from conf.configure import Configure\n",
    "from utils import data_utils, dataframe_util\n",
    "from utils.common_utils import common_num_range\n",
    "\n",
    "import model.get_datasets as gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 40307, test: 10076, feature count: 368, orderType 1:0 = 0.16436\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(Configure.base_path + 'huang_lin/train_dataHL.csv')\n",
    "test = pd.read_csv(Configure.base_path + 'huang_lin/test_dataHL.csv')\n",
    "\n",
    "y_train = train['orderType']\n",
    "train.drop(['orderType'], axis=1, inplace=True)\n",
    "\n",
    "df_columns = train.columns.values\n",
    "print('train: {}, test: {}, feature count: {}, orderType 1:0 = {:.5f}'.format(\n",
    "    train.shape[0], test.shape[0], len(df_columns), 1.0*sum(y_train) / len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1643635100602873"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train.values, y_train, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(test, feature_names=df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "The overall parameters can be divided into 3 categories:\n",
    "\n",
    "### General Parameters: Guide the overall functioning\n",
    "1. booster [default=gbtree], Select the type of model to run at each iteration. It has 2 options:\n",
    "- gbtree: tree-based models\n",
    "- gblinear: linear models\n",
    "\n",
    "2. silent [default=0]:\n",
    "- Silent mode is activated is set to 1, i.e. no running messages will be printed.\n",
    "- It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "3. nthread [default to maximum number of threads available if not set]\n",
    "\n",
    "### Booster Parameters : Guide the individual booster (tree/regression) at each step\n",
    "Though there are 2 types of boosters, I’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "1.eta [default=0.3]\n",
    "    - Analogous to **learning rate** in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "    \n",
    "2. min_child_weight [default=1]\n",
    "    - Defines **the minimum sum of weights of all observations required in a child**.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "3. max_depth [default=6]\n",
    "    - **The maximum depth of a tree**, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.Should be tuned using CV.Typical values: 3-10\n",
    "\n",
    "4. max_leaf_nodes\n",
    "    - **The maximum number of terminal nodes or leaves in a tree**.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - ** If this is defined, GBM will ignore max_depth**.\n",
    "\n",
    "5. gamma [default=0]\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. **Gamma specifies the minimum loss reduction required to make a split**.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    - The higher Gamma is, the higher the regularization. Default value is 0 (no regularization).\n",
    "\n",
    "6. max_delta_step [default=0]\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "\n",
    "7. subsample [default=1]\n",
    "    - Same as the subsample of GBM. Denotes **the fraction of observations to be randomly samples for each tree**.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "\n",
    "8. colsample_bytree [default=1]\n",
    "    - Similar to max_features in GBM. Denotes **the fraction of columns to be randomly samples for each tree**.\n",
    "    - Typical values: 0.5-1\n",
    "\n",
    "9. colsample_bylevel [default=1]\n",
    "    - Denotes **the subsample ratio of columns for each split, in each level**.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "\n",
    "10. lambda [default=1]\n",
    "    - **L2 regularization term on weights** (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "\n",
    "11. alpha [default=0]\n",
    "    - **L1 regularization term on weights** (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "\n",
    "12. scale_pos_weight [default=1]\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "\n",
    "### Learning Task Parameters: Guide the optimization performed\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "1. objective [default=reg:linear]\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "            - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "\n",
    "2. eval_metric [ default according to objective ]\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "        - rmse – root mean square error\n",
    "        - mae – mean absolute error\n",
    "        - logloss – negative log-likelihood\n",
    "        - error – Binary classification error rate (0.5 threshold)\n",
    "        - merror – Multiclass classification error rate\n",
    "        - mlogloss – Multiclass logloss\n",
    "        - auc: Area under the curve\n",
    "\n",
    "3. seed [default=0]\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Approach for Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various steps to be performed are:\n",
    "\n",
    "1. Choose a **relatively high learning rate**. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. **Determine the optimum number of trees for this learning rate**. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "2. **Tune tree-specific parameters** (*max_depth, min_child_weight, gamma, subsample, colsample_bytree*) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "3. **Tune regularization parameters** (*lambda, alpha*) for xgboost which can help reduce model complexity and enhance performance.\n",
    "4. Lower the learning rate and decide the optimal parameters ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from itertools import product\n",
    "\n",
    "def model_cross_validate(xgb_params, cv_param_dict, dtrain, cv_num_boost_round=4000, early_stopping_rounds=100, cv_nfold=5, stratified=True):\n",
    "    params_value = []\n",
    "    params_name = cv_param_dict.keys()\n",
    "    max_auc = 0\n",
    "    for param in params_name:\n",
    "        params_value.append(cv_param_dict[param])\n",
    "\n",
    "    for param_pair in product(*params_value):\n",
    "        param_str = ''\n",
    "        for i in xrange(len(param_pair)):\n",
    "            param_str += params_name[i] + '=' + str(param_pair[i]) + ' '\n",
    "            xgb_params[params_name[i]] = param_pair[i]\n",
    "        \n",
    "        start = time.time()\n",
    "        cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=cv_num_boost_round, stratified=stratified,\n",
    "                           nfold=cv_nfold, early_stopping_rounds=early_stopping_rounds)\n",
    "        \n",
    "        best_num_boost_rounds = len(cv_result)\n",
    "        mean_test_auc = cv_result.loc[best_num_boost_rounds - 6: best_num_boost_rounds - 1, 'test-auc-mean'].mean()\n",
    "        if mean_test_auc > max_auc:\n",
    "            best_param = param_pair\n",
    "            max_auc = mean_test_auc\n",
    "        \n",
    "        end = time.time()\n",
    "        print('Tuning paramter: {}, best_ntree_limit:{}, auc = {:.7f}, cost: {}s'.format(param_str, best_num_boost_rounds,\n",
    "                                                                              mean_test_auc, end-start))\n",
    "    param_str = ''\n",
    "    for i in xrange(len(best_param)):\n",
    "        param_str += params_name[i] + '=' + str(best_param[i]) + ' '\n",
    "        xgb_params[params_name[i]] = best_param[i]\n",
    "    print('===========best paramter: {} auc={:.7f}==========='.format(param_str, max_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fix learning rate and number of estimators for tuning tree-based parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'scale_pos_weight': 1,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic',\n",
    "    'updater': 'grow_gpu',\n",
    "    'gpu_id':0,\n",
    "    'nthread': -1,\n",
    "    'silent': 1,\n",
    "    'booster': 'gbtree',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> calc baseline model\n",
      "mean_train_auc = 0.9974385 , mean_test_auc = 0.9691350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('---> calc baseline model')\n",
    "\n",
    "cv_num_boost_round=4000\n",
    "early_stopping_rounds=100\n",
    "cv_nfold=5\n",
    "stratified=True\n",
    "\n",
    "cv_result = xgb.cv(xgb_params,\n",
    "                   dtrain,\n",
    "                   nfold=cv_nfold,\n",
    "                   stratified=stratified,\n",
    "                   num_boost_round=cv_num_boost_round,\n",
    "                   early_stopping_rounds=early_stopping_rounds,\n",
    "                   )\n",
    "best_num_boost_rounds = len(cv_result)\n",
    "mean_train_auc = cv_result.loc[best_num_boost_rounds-6 : best_num_boost_rounds-1, 'train-auc-mean'].mean()\n",
    "mean_test_auc = cv_result.loc[best_num_boost_rounds-6 : best_num_boost_rounds-1, 'test-auc-mean'].mean()\n",
    "\n",
    "print('mean_train_auc = {:.7f} , mean_test_auc = {:.7f}\\n'.format(mean_train_auc, mean_test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune *max_depth* and *min_child_weight*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning paramter: max_depth=5 min_child_weight=1 , best_ntree_limit:368, auc = 0.9691350, cost: 90.7996020317s\n",
      "Tuning paramter: max_depth=5 min_child_weight=3 , best_ntree_limit:388, auc = 0.9698913, cost: 94.5384869576s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c47bd9e99d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv_paramters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_cross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_paramters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-901e9fa71cf8>\u001b[0m in \u001b[0;36mmodel_cross_validate\u001b[0;34m(xgb_params, cv_param_dict, dtrain, cv_num_boost_round, early_stopping_rounds, cv_nfold, stratified)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=cv_num_boost_round, stratified=stratified,\n\u001b[0;32m---> 21\u001b[0;31m                            nfold=cv_nfold, early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbest_num_boost_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    405\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_paramters = {'max_depth':range(5,15,2),'min_child_weight':range(1,10,2)}\n",
    "model_cross_validate(xgb_params, cv_paramters, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning paramter: max_depth=10 min_child_weight=2 , best_ntree_limit:607, auc = 0.9692929, cost: 507.173731089s\n",
      "Tuning paramter: max_depth=10 min_child_weight=3 , best_ntree_limit:434, auc = 0.9687700, cost: 375.949675083s\n",
      "Tuning paramter: max_depth=10 min_child_weight=4 , best_ntree_limit:348, auc = 0.9691484, cost: 313.063239098s\n",
      "Tuning paramter: max_depth=11 min_child_weight=2 , best_ntree_limit:601, auc = 0.9696587, cost: 547.44369483s\n",
      "Tuning paramter: max_depth=11 min_child_weight=3 , best_ntree_limit:596, auc = 0.9697522, cost: 541.135313034s\n",
      "Tuning paramter: max_depth=11 min_child_weight=4 , best_ntree_limit:367, auc = 0.9696156, cost: 366.118253946s\n",
      "Tuning paramter: max_depth=12 min_child_weight=2 , best_ntree_limit:974, auc = 0.9694576, cost: 949.071660042s\n",
      "Tuning paramter: max_depth=12 min_child_weight=3 , best_ntree_limit:460, auc = 0.9696810, cost: 502.434431076s\n",
      "Tuning paramter: max_depth=12 min_child_weight=4 , best_ntree_limit:497, auc = 0.9694188, cost: 533.029690981s\n",
      "===========best paramter: max_depth=11 min_child_weight=3  auc=0.9697522===========\n"
     ]
    }
   ],
   "source": [
    "cv_paramters = {'max_depth':range(10,13,1),'min_child_weight':range(2,5,1)}\n",
    "model_cross_validate(xgb_params, cv_paramters, dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_paramters={'gamma':common_num_range(0,10,1)}\n",
    "# model_cross_validate(xgb_params, cv_paramters, dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning paramter: subsample=0.5 colsample_bytree=0.5 , best_ntree_limit:223, auc = 0.9679042, cost: 222.292212009s\n",
      "Tuning paramter: subsample=0.5 colsample_bytree=0.7 , best_ntree_limit:577, auc = 0.9670692, cost: 502.259080172s\n",
      "Tuning paramter: subsample=0.5 colsample_bytree=0.9 , best_ntree_limit:326, auc = 0.9678104, cost: 377.781816959s\n",
      "Tuning paramter: subsample=0.7 colsample_bytree=0.5 , best_ntree_limit:368, auc = 0.9690405, cost: 462.038894176s\n",
      "Tuning paramter: subsample=0.7 colsample_bytree=0.7 , best_ntree_limit:327, auc = 0.9691685, cost: 569.801518917s\n",
      "Tuning paramter: subsample=0.7 colsample_bytree=0.9 , best_ntree_limit:632, auc = 0.9687758, cost: 1015.72838306s\n",
      "Tuning paramter: subsample=0.9 colsample_bytree=0.5 , best_ntree_limit:595, auc = 0.9700122, cost: 740.865426064s\n",
      "Tuning paramter: subsample=0.9 colsample_bytree=0.7 , best_ntree_limit:758, auc = 0.9697036, cost: 719.50315094s\n",
      "Tuning paramter: subsample=0.9 colsample_bytree=0.9 , best_ntree_limit:750, auc = 0.9693994, cost: 773.213275909s\n",
      "===========best paramter: subsample=0.9 colsample_bytree=0.5  auc=0.9700122===========\n"
     ]
    }
   ],
   "source": [
    "cv_paramters = {'subsample':common_num_range(0.5, 1, 0.2), 'colsample_bytree':common_num_range(0.5,1,0.2)}\n",
    "model_cross_validate(xgb_params,cv_paramters,dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning paramter: subsample=0.8 colsample_bytree=0.4 , best_ntree_limit:648, auc = 0.9694050, cost: 550.568568945s\n",
      "Tuning paramter: subsample=0.8 colsample_bytree=0.5 , best_ntree_limit:652, auc = 0.9697718, cost: 582.018703938s\n",
      "Tuning paramter: subsample=0.8 colsample_bytree=0.6 , best_ntree_limit:809, auc = 0.9695466, cost: 731.63205409s\n",
      "Tuning paramter: subsample=0.9 colsample_bytree=0.4 , best_ntree_limit:515, auc = 0.9698373, cost: 417.920390844s\n",
      "Tuning paramter: subsample=0.9 colsample_bytree=0.5 , best_ntree_limit:595, auc = 0.9700122, cost: 500.100728989s\n",
      "Tuning paramter: subsample=0.9 colsample_bytree=0.6 , best_ntree_limit:639, auc = 0.9696043, cost: 554.651578903s\n",
      "Tuning paramter: subsample=1.0 colsample_bytree=0.4 , best_ntree_limit:543, auc = 0.9696669, cost: 429.173371792s\n",
      "Tuning paramter: subsample=1.0 colsample_bytree=0.5 , best_ntree_limit:1046, auc = 0.9697188, cost: 807.565859079s\n",
      "Tuning paramter: subsample=1.0 colsample_bytree=0.6 , best_ntree_limit:742, auc = 0.9696306, cost: 628.740350008s\n",
      "===========best paramter: subsample=0.9 colsample_bytree=0.5  auc=0.9700122===========\n"
     ]
    }
   ],
   "source": [
    "cv_paramters = {'subsample':common_num_range(0.8, 1.1, 0.1), 'colsample_bytree':common_num_range(0.4,0.7,0.1)}\n",
    "model_cross_validate(xgb_params,cv_paramters,dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters: alpha, lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning paramter: alpha=1e-05 lambda=1e-05 , best_ntree_limit:503, auc = 0.9692792, cost: 425.94354701s\n",
      "Tuning paramter: alpha=1e-05 lambda=0.001 , best_ntree_limit:467, auc = 0.9695275, cost: 416.013977051s\n",
      "Tuning paramter: alpha=1e-05 lambda=0.01 , best_ntree_limit:689, auc = 0.9692839, cost: 563.177620888s\n",
      "Tuning paramter: alpha=1e-05 lambda=0.1 , best_ntree_limit:670, auc = 0.9695547, cost: 528.730576992s\n",
      "Tuning paramter: alpha=1e-05 lambda=1 , best_ntree_limit:923, auc = 0.9701873, cost: 698.041824818s\n",
      "Tuning paramter: alpha=1e-05 lambda=10 , best_ntree_limit:453, auc = 0.9702873, cost: 391.562325001s\n",
      "Tuning paramter: alpha=1e-05 lambda=100 , best_ntree_limit:561, auc = 0.9696862, cost: 474.354606152s\n",
      "Tuning paramter: alpha=0.001 lambda=1e-05 , best_ntree_limit:692, auc = 0.9692623, cost: 544.138518095s\n",
      "Tuning paramter: alpha=0.001 lambda=0.001 , best_ntree_limit:628, auc = 0.9693156, cost: 509.361008883s\n",
      "Tuning paramter: alpha=0.001 lambda=0.01 , best_ntree_limit:376, auc = 0.9690601, cost: 347.107156992s\n",
      "Tuning paramter: alpha=0.001 lambda=0.1 , best_ntree_limit:509, auc = 0.9692287, cost: 421.79590106s\n",
      "Tuning paramter: alpha=0.001 lambda=1 , best_ntree_limit:636, auc = 0.9698130, cost: 507.119144917s\n",
      "Tuning paramter: alpha=0.001 lambda=10 , best_ntree_limit:572, auc = 0.9700865, cost: 471.647753s\n",
      "Tuning paramter: alpha=0.001 lambda=100 , best_ntree_limit:660, auc = 0.9696986, cost: 543.405856133s\n",
      "Tuning paramter: alpha=0.01 lambda=1e-05 , best_ntree_limit:565, auc = 0.9691912, cost: 458.038650036s\n",
      "Tuning paramter: alpha=0.01 lambda=0.001 , best_ntree_limit:672, auc = 0.9693227, cost: 529.934556961s\n",
      "Tuning paramter: alpha=0.01 lambda=0.01 , best_ntree_limit:643, auc = 0.9692237, cost: 509.896362066s\n",
      "Tuning paramter: alpha=0.01 lambda=0.1 , best_ntree_limit:665, auc = 0.9696939, cost: 527.38176012s\n",
      "Tuning paramter: alpha=0.01 lambda=1 , best_ntree_limit:757, auc = 0.9700250, cost: 592.431385994s\n",
      "Tuning paramter: alpha=0.01 lambda=10 , best_ntree_limit:675, auc = 0.9702642, cost: 544.064417839s\n",
      "Tuning paramter: alpha=0.01 lambda=100 , best_ntree_limit:624, auc = 0.9698011, cost: 509.219565153s\n",
      "Tuning paramter: alpha=0.1 lambda=1e-05 , best_ntree_limit:610, auc = 0.9693676, cost: 480.763124943s\n",
      "Tuning paramter: alpha=0.1 lambda=0.001 , best_ntree_limit:660, auc = 0.9697333, cost: 512.820445776s\n",
      "Tuning paramter: alpha=0.1 lambda=0.01 , best_ntree_limit:770, auc = 0.9699251, cost: 585.112503052s\n",
      "Tuning paramter: alpha=0.1 lambda=0.1 , best_ntree_limit:859, auc = 0.9695508, cost: 647.53328681s\n",
      "Tuning paramter: alpha=0.1 lambda=1 , best_ntree_limit:504, auc = 0.9697304, cost: 414.133984089s\n",
      "Tuning paramter: alpha=0.1 lambda=10 , best_ntree_limit:328, auc = 0.9700341, cost: 300.774034023s\n",
      "Tuning paramter: alpha=0.1 lambda=100 , best_ntree_limit:596, auc = 0.9696384, cost: 492.3832798s\n",
      "Tuning paramter: alpha=1 lambda=1e-05 , best_ntree_limit:436, auc = 0.9696092, cost: 371.959009171s\n",
      "Tuning paramter: alpha=1 lambda=0.001 , best_ntree_limit:640, auc = 0.9695629, cost: 508.987262964s\n",
      "Tuning paramter: alpha=1 lambda=0.01 , best_ntree_limit:371, auc = 0.9694289, cost: 328.519499063s\n",
      "Tuning paramter: alpha=1 lambda=0.1 , best_ntree_limit:649, auc = 0.9697050, cost: 514.896018982s\n",
      "Tuning paramter: alpha=1 lambda=1 , best_ntree_limit:385, auc = 0.9699348, cost: 338.105120897s\n",
      "Tuning paramter: alpha=1 lambda=10 , best_ntree_limit:581, auc = 0.9700795, cost: 475.781857967s\n",
      "Tuning paramter: alpha=1 lambda=100 , best_ntree_limit:581, auc = 0.9695357, cost: 482.998793125s\n",
      "Tuning paramter: alpha=10 lambda=1e-05 , best_ntree_limit:321, auc = 0.9692712, cost: 296.513639927s\n",
      "Tuning paramter: alpha=10 lambda=0.001 , best_ntree_limit:372, auc = 0.9690904, cost: 331.250604868s\n",
      "Tuning paramter: alpha=10 lambda=0.01 , best_ntree_limit:380, auc = 0.9692437, cost: 336.49017787s\n",
      "Tuning paramter: alpha=10 lambda=0.1 , best_ntree_limit:383, auc = 0.9690616, cost: 338.455038071s\n",
      "Tuning paramter: alpha=10 lambda=1 , best_ntree_limit:440, auc = 0.9690097, cost: 377.678958893s\n",
      "Tuning paramter: alpha=10 lambda=10 , best_ntree_limit:660, auc = 0.9691656, cost: 530.711436987s\n",
      "Tuning paramter: alpha=10 lambda=100 , best_ntree_limit:729, auc = 0.9691664, cost: 579.350821972s\n",
      "Tuning paramter: alpha=100 lambda=1e-05 , best_ntree_limit:1771, auc = 0.9592564, cost: 1195.087044s\n",
      "Tuning paramter: alpha=100 lambda=0.001 , best_ntree_limit:444, auc = 0.9584424, cost: 356.251420021s\n",
      "Tuning paramter: alpha=100 lambda=0.01 , best_ntree_limit:1547, auc = 0.9589735, cost: 1053.59861588s\n",
      "Tuning paramter: alpha=100 lambda=0.1 , best_ntree_limit:1476, auc = 0.9590704, cost: 1009.85384393s\n",
      "Tuning paramter: alpha=100 lambda=1 , best_ntree_limit:1812, auc = 0.9591803, cost: 1220.89043689s\n",
      "Tuning paramter: alpha=100 lambda=10 , best_ntree_limit:826, auc = 0.9588733, cost: 597.933846951s\n",
      "Tuning paramter: alpha=100 lambda=100 , best_ntree_limit:1988, auc = 0.9591190, cost: 1334.99804211s\n",
      "===========best paramter: alpha=1e-05 lambda=10  auc=0.9702873===========\n"
     ]
    }
   ],
   "source": [
    "cv_paramters = {'alpha':[1e-5, 1e-3, 1e-2, 0.1, 1, 10, 100],\n",
    "                'lambda':[1e-5, 1e-3, 1e-2, 0.1, 1, 10, 100]}\n",
    "model_cross_validate(xgb_params,cv_paramters,dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Learning Rate and Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-05,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bytree': 0.5,\n",
       " 'eta': 0.1,\n",
       " 'eval_metric': 'auc',\n",
       " 'gamma': 4,\n",
       " 'gpu_id': 0,\n",
       " 'lambda': 10,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 3,\n",
       " 'nthread': -1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'scale_pos_weight': 1,\n",
       " 'silent': 1,\n",
       " 'subsample': 0.9,\n",
       " 'updater': 'grow_gpu'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params['eta'] = 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
